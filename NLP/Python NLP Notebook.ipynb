{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import docx2txt\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import shutil\n",
    "import contractions\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from git to clean up expressions\n",
    "def unicodetoascii(text):\n",
    "\n",
    "    TEXT = (text.\n",
    "    \t\treplace('\\\\xe2\\\\x80\\\\x99', \"'\").\n",
    "            replace('\\\\xc3\\\\xa9', 'e').\n",
    "            replace('\\\\xe2\\\\x80\\\\x90', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x91', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x92', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x93', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x94', '-').\n",
    "            replace('\\\\xe2\\\\x80\\\\x98', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9b', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9c', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9d', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9e', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\x9f', '\"').\n",
    "            replace('\\\\xe2\\\\x80\\\\xa6', '...').\n",
    "            replace('\\\\xe2\\\\x80\\\\xb2', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb3', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb4', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb5', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb6', \"'\").\n",
    "            replace('\\\\xe2\\\\x80\\\\xb7', \"'\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xba', \"+\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbb', \"-\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbc', \"=\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbd', \"(\").\n",
    "            replace('\\\\xe2\\\\x81\\\\xbe', \")\")\n",
    "\n",
    "                 )\n",
    "    return TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize word\n",
    "nltk.download('punkt')\n",
    "text = textract.process(r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\Argo Archive Word\\converted word documents\\1-28-1949 Leela Menon(scan)pdf_00001.docx\")\n",
    "text = str(text)\n",
    "text = unicodetoascii(text.replace(\"\\\\n\", \"\\n\")) #clean up the doc\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by year\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "tokenized_word = word_tokenize(text)\n",
    "year = re.findall(r'.*([1-3][0-9]{3})', text)\n",
    "\n",
    "sorted_year = dict()\n",
    "file_length = []\n",
    "directory = f\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\Argo Archive Text\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    with open(f, encoding= \"utf8\") as text_file:\n",
    "        file_str = text_file.read()\n",
    "        years = re.findall(r'.*([1][89][890123456][0-9])', file_str)\n",
    "    for year in years:\n",
    "        if int(year) > 1889:\n",
    "            if year not in sorted_year.keys():\n",
    "                this_list = [str(f)]\n",
    "                sorted_year[year] = this_list\n",
    "            else:\n",
    "                sorted_year[year].append(str(f))\n",
    "\n",
    "#print(sorted_year)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0-00-000 First Name Last Name (scan)_00001.txt', 'Delete pls, faulty scan_00001.txt']\n"
     ]
    }
   ],
   "source": [
    "#new sorted year function\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "error_list = list()\n",
    "def sort_year():\n",
    "    mother_directory = r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\ArgoArchive\\Argo Archive Text\"\n",
    "    for filename in os.listdir(mother_directory):\n",
    "        \n",
    "        yearNumber = re.findall(r'.*([1][89][890123456][0-9])', filename)\n",
    "        if len(yearNumber)!= 0:\n",
    "            goal_directory = r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\ArgoArchive\\Argo Archive Text in sorted years\" + \"\\\\\" + yearNumber[0]\n",
    "            if os.path.exists(goal_directory):\n",
    "                \n",
    "                src_file = mother_directory + \"\\\\\" + filename\n",
    "                dst_file = goal_directory + \"\\\\\" + filename.split(\".\")[0] + \"2nd copy.text\"\n",
    "                shutil.copy(src_file, dst_file)\n",
    "            else:\n",
    "                os.makedirs(goal_directory)\n",
    "                src_file = mother_directory + \"\\\\\" + filename\n",
    "                dst_file = goal_directory + \"\\\\\" + filename.split(\".\")[0] + \"2nd copy.text\"\n",
    "                shutil.copy(src_file, dst_file)\n",
    "        else:\n",
    "            error_list.append(filename)\n",
    "    \n",
    "        \n",
    "        \n",
    "sort_year()\n",
    "print(error_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. create a folder name after the pdf file\n",
    "2. create another folder for every page of the pdf file\n",
    "3. separate the pdf file and put it in the same folder\n",
    "4. ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\ArgoArchive\\Argo Archive Text in sorted years\"\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort files in to separate directories using python\n",
    "mother_dir = r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\newspaper text in sorted years\"\n",
    "for i in sorted_year.keys():\n",
    "    newpath = i\n",
    "    newfolder = os.path.join(mother_dir, i)\n",
    "    if not os.path.exists(newfolder):\n",
    "        os.makedirs(newfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mother_dir = r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\Newspaper text in sorted years\"\n",
    "\n",
    "for i in sorted_year.keys():\n",
    "    parent_dir = mother_dir + \"\\\\\" + i\n",
    "    #print(parent_dir)\n",
    "    for src_file_path in sorted_year[i]:\n",
    "        #print(src_file_path)\n",
    "        dst_file_path = parent_dir + \"\\\\\" + src_file_path.split(\"\\\\\")[-1].split(\".\")[0] + \"(copy).txt\"\n",
    "        #print(dst_file_path)\n",
    "        shutil.copy(src_file_path, dst_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Cheng\\OneDrive\\桌面\\WIN History Project\\ArgoArchive\\Argo Archive Text\\1-19-1938 Leela Menon(scan)_00001.txt\", encoding= \"utf8\") as text:\n",
    "    text = text.read()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Opening up in the first min-\n",
    "utes of play, on a foul counter\n",
    "by Jack Bartow, Rutgers Prep\n",
    "went on to win the seconed game\n",
    "of the season in as many starts.\n",
    "\n",
    "The Prep attack was centered around the mighty mite forward, Joey Tretsky, who found the\n",
    "mark from all angles of the court,\n",
    "\n",
    "The score at the half saw\n",
    "Prep ahead 20-12, with Tretsky\n",
    "and Harper taking command of the\n",
    "scoring. We really opened up in\n",
    "the third period by making twenty-one points, and went wild with\n",
    "the one-hand shots of John,\"Ivan\n",
    "The Terrible\" ,Kluey and the com-\n",
    "bination cut-in shots of George\n",
    "\"Powerhouse\" Burton, Bill \"Slim\"\n",
    "Harper, and Jack \"Dainty Gloves\"\n",
    "Bartow.\n",
    "\n",
    "During the last frame the\n",
    "entire reserve team replaced the\n",
    "\"Big Five\". In this period Prep\n",
    "saw that their own Joel \"Pop\"\n",
    "Fertig had not lost his shooting\n",
    "eye as a result of the first game.\n",
    "\n",
    "Scoring honors went to Joey\n",
    "Tretsky, who rang up six field\n",
    "goals and to John Kluey with four\n",
    "field goals and three foul points'''\n",
    "\n",
    "\n",
    "text = text.replace(\"\\n\", \" \")\n",
    "text = \" \".join(text.split())\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "\n",
    "this_list = [\"\"\"Opening up in the first min- utes of play, on a foul counter by Jack Bartow, Rutgers Prep went on to win the seconed game of the season in as many starts. The Prep attack was centered around the mighty mite forward, Joey Tretsky, who found the mark from all angles of the court, The score at the half saw Prep ahead 20-12, with Tretsky and Harper taking command of the scoring. We really opened up in the third period by making twenty-one points, and went wild with the one-hand shots of John,\"Ivan The Terrible\" ,Kluey and the com- bination cut-in shots of George \"Powerhouse\" Burton, Bill \"Slim\" Harper, and Jack \"Dainty Gloves\" Bartow. During the last frame the entire reserve team replaced the \"Big Five\". In this period Prep saw that their own Joel \"Pop\" Fertig had not lost his shooting eye as a result of the first game. Scoring honors went to Joey Tretsky, who rang up six field goals and to John Kluey with four field goals and three foul points\"\"\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "max_length = 2000\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "batch = tokenizer(this_list, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(device)\n",
    "translated = model.generate(**batch, max_length=max_length)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(tgt_text[0])\n",
    "#string processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Prep attack was centered around the mighty mite forward, Joey Tretsky, who found the mark from all angles of the court . The score at the half saw Prep ahead 20-12,  with Tretsky and Harper taking command of the scoring .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "## Setting to use the 0th GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## Setting to use the bart-large-cnn model for summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "text = \"\"\"Opening up in the first min- utes of play, on a foul counter by Jack Bartow, \n",
    "Rutgers Prep went on to win the seconed game of the season in as many starts. The Prep attack was centered around the mighty mite forward, \n",
    "Joey Tretsky, who found the mark from all angles of the court, The score at the half saw Prep ahead 20-12, \n",
    "with Tretsky and Harper taking command of the scoring. We really opened up in the third period by making twenty-one points, \n",
    "and went wild with the one-hand shots of John,\"Ivan The Terrible\" ,Kluey and the com- bination cut-in shots of George \"Powerhouse\" Burton, \n",
    "Bill \"Slim\" Harper, and Jack \"Dainty Gloves\" Bartow. During the last frame the entire reserve team replaced the \"Big Five\". \n",
    "In this period Prep saw that their own Joel \"Pop\" Fertig had not lost his shooting eye as a result of the first game. \n",
    "Scoring honors went to Joey Tretsky, who rang up six field goals and to John Kluey with four field goals and three foul points\"\"\"\n",
    "\n",
    "summary_text = summarizer(text, max_length=100, min_length=35, do_sample=False)[0]['summary_text']\n",
    "print(summary_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbb24439d0f377cc9439056959e67499e213ea146e61dd405f0e49a9ad6cbeac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
