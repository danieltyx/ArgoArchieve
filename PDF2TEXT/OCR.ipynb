{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CHATGPT\n",
    "You are correct that the OCR process typically reads from left to right and may not be able to properly recognize text in separate columns of a newspaper. Here are a few suggestions for how you can modify the OCR process to better handle multi-column text in a newspaper:\n",
    "\n",
    "Split the image into separate columns: One option is to split the image into separate columns and perform OCR on each column individually. You can use image processing techniques such as edge detection or thresholding to detect the column boundaries and then split the image into separate images for each column.\n",
    "\n",
    "Use OCR software that is designed for multi-column text: Some OCR software, such as ABBYY FlexiCapture, is specifically designed to handle multi-column text and may provide better results in this case. You can try using one of these specialized OCR tools instead of pytesseract.\n",
    "\n",
    "Train a custom OCR model: Another option is to train a custom OCR model using machine learning techniques. You can use a dataset of scanned newspaper images with corresponding transcriptions to train a model that is able to recognize multi-column text. This approach requires a more in-depth understanding of machine learning and OCR, but it may provide the best results for your specific use case.\n",
    "\n",
    "I hope these suggestions help! Let me know if you have any further questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'box'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Perform OCR and extract the bounding boxes of the words\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m boxes \u001b[39m=\u001b[39m pytesseract\u001b[39m.\u001b[39;49mimage_to_data(gray, output_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbox\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Split the boxes into lines based on the y-coordinate\u001b[39;00m\n\u001b[0;32m     15\u001b[0m lines \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pytesseract\\pytesseract.py:527\u001b[0m, in \u001b[0;36mimage_to_data\u001b[1;34m(image, lang, config, nice, output_type, timeout, pandas_config)\u001b[0m\n\u001b[0;32m    524\u001b[0m config \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m-c tessedit_create_tsv=1 \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mstrip()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    525\u001b[0m args \u001b[39m=\u001b[39m [image, \u001b[39m'\u001b[39m\u001b[39mtsv\u001b[39m\u001b[39m'\u001b[39m, lang, config, nice, timeout]\n\u001b[1;32m--> 527\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    528\u001b[0m     Output\u001b[39m.\u001b[39;49mBYTES: \u001b[39mlambda\u001b[39;49;00m: run_and_get_output(\u001b[39m*\u001b[39;49m(args \u001b[39m+\u001b[39;49m [\u001b[39mTrue\u001b[39;49;00m])),\n\u001b[0;32m    529\u001b[0m     Output\u001b[39m.\u001b[39;49mDATAFRAME: \u001b[39mlambda\u001b[39;49;00m: get_pandas_output(\n\u001b[0;32m    530\u001b[0m         args \u001b[39m+\u001b[39;49m [\u001b[39mTrue\u001b[39;49;00m],\n\u001b[0;32m    531\u001b[0m         pandas_config,\n\u001b[0;32m    532\u001b[0m     ),\n\u001b[0;32m    533\u001b[0m     Output\u001b[39m.\u001b[39;49mDICT: \u001b[39mlambda\u001b[39;49;00m: file_to_dict(run_and_get_output(\u001b[39m*\u001b[39;49margs), \u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    534\u001b[0m     Output\u001b[39m.\u001b[39;49mSTRING: \u001b[39mlambda\u001b[39;49;00m: run_and_get_output(\u001b[39m*\u001b[39;49margs),\n\u001b[0;32m    535\u001b[0m }[output_type]()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'box'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Read the PDF file\n",
    "pdf_file = r\"C:\\Users\\Cheng\\Downloads\\1-19-1938 Leela Menon(scan).png\"\n",
    "image = cv2.imread(pdf_file)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform OCR and extract the bounding boxes of the words\n",
    "boxes = pytesseract.image_to_data(gray, output_type='box')\n",
    "\n",
    "# Split the boxes into lines based on the y-coordinate\n",
    "lines = []\n",
    "for b in boxes:\n",
    "    # Extract the bounding box coordinates and text\n",
    "    x, y, w, h = b['x'], b['y'], b['w'], b['h']\n",
    "    x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "    \n",
    "    # Check if the word belongs to a new line\n",
    "    if len(lines) == 0 or y > lines[-1][-1][1]:\n",
    "        # Start a new line\n",
    "        lines.append([(x, y, w, h)])\n",
    "    else:\n",
    "        # Add the word to the current line\n",
    "        lines[-1].append((x, y, w, h))\n",
    "\n",
    "# Identify the boundaries of different text sections\n",
    "section_boundaries = []\n",
    "for i, line in enumerate(lines):\n",
    "    # Calculate the mean y-coordinate of the line\n",
    "    y = int(sum([b[1] for b in line]) / len(line))\n",
    "    \n",
    "    # Check if the line is the first or last line in the page\n",
    "    if i == 0 or i == len(lines) - 1:\n",
    "        # Add the line as a boundary\n",
    "        section_boundaries.append(y)\n",
    "    else:\n",
    "        # Check if the line is significantly different in y-coordinate from the previous line\n",
    "        if abs(y - lines[i-1][0][1]) > 20:\n",
    "            # Add the line as a boundary\n",
    "            section_boundaries.append(y)\n",
    "\n",
    "# Draw the section boundaries on the image\n",
    "for y in section_boundaries:\n",
    "    cv2.line(image, (0, y), (image.shape[1], y), (0, 0, 255), 2)\n",
    "\n",
    "# Display the image with the bounding boxes\n",
    "cv2.imshow(\"Text Layout\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page segmentation modes:\n",
      "  0    Orientation and script detection (OSD) only.\n",
      "  1    Automatic page segmentation with OSD.\n",
      "  2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
      "  3    Fully automatic page segmentation, but no OSD. (Default)\n",
      "  4    Assume a single column of text of variable sizes.\n",
      "  5    Assume a single uniform block of vertically aligned text.\n",
      "  6    Assume a single uniform block of text.\n",
      "  7    Treat the image as a single text line.\n",
      "  8    Treat the image as a single word.\n",
      "  9    Treat the image as a single word in a circle.\n",
      " 10    Treat the image as a single character.\n",
      " 11    Sparse text. Find as much text as possible in no particular order.\n",
      " 12    Sparse text with OSD.\n",
      " 13    Raw line. Treat the image as a single text line,\n",
      "       bypassing hacks that are Tesseract-specific.\n"
     ]
    }
   ],
   "source": [
    "!tesseract --help-psm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "\n",
    "\n",
    "pdf_file = r\"C:\\Users\\Cheng\\Downloads\\1-19-1938 Leela Menon(scan).png\"\n",
    "image = cv2.imread(pdf_file)\n",
    "\n",
    "\n",
    "data = pytesseract.image_to_data(image, output_type='data.frame', config=\"--psm 1\")\n",
    "\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    \n",
    "    if row['text'] != '':\n",
    "        \n",
    "        x, y, w, h = row['left'], row['top'], row['width'], row['height']\n",
    "        \n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "cv2.imwrite(r\"C:\\Users\\Cheng\\Downloads\\Open Peeps - Avatar.png\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbb24439d0f377cc9439056959e67499e213ea146e61dd405f0e49a9ad6cbeac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
